{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e7376a8-edf9-42da-b94d-05991f40f79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt': 100% ━━━━━━━━━━━━ 6.2MB 3.4MB/s 1.8s.8s<0.0s<1.5s\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "model = YOLO(\"yolov8n.pt\") \n",
    "\n",
    "# Create folders to save detections\n",
    "os.makedirs(\"frames_in\", exist_ok=True)\n",
    "os.makedirs(\"frames_out\", exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c3840b2-d14f-47e4-8923-f00195da6b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Person Detection in each video:\n",
    "\n",
    "def extract_person_detections(video_path, output_folder, conf_threshold=0.5):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_idx = 0\n",
    "    detections = {}\n",
    "\n",
    "    while True:\n",
    "        ret,frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        results = model(frame, conf=conf_threshold, verbose=False)\n",
    "        boxes = []\n",
    "        for r in results[0].boxes:\n",
    "            cls = int(r.cls)\n",
    "            if cls == 0: \n",
    "                x1, y1, x2, y2 = map(int, r.xyxy[0])\n",
    "                boxes.append([x1, y1, x2, y2])\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "        detections[frame_idx] = boxes\n",
    "        cv2.imwrite(f\"{output_folder}/frame_{frame_idx:05d}.jpg\", frame)\n",
    "        frame_idx += 1\n",
    "\n",
    "    cap.release()\n",
    "    return detections\n",
    "\n",
    "# Detect people in both videos\n",
    "in_detections = extract_person_detections(\"In.mp4\", \"frames_in\")\n",
    "out_detections = extract_person_detections(\"Out.mp4\", \"frames_out\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c31e436-a552-413f-a333-a19f58f14088",
   "metadata": {},
   "outputs": [],
   "source": [
    "#person_Identification\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "import timm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "# Load resnet50 model for embeddings\n",
    "reid_model = timm.create_model(\"resnet50\", pretrained=True)\n",
    "reid_model.fc = nn.Identity()\n",
    "reid_model = reid_model.to(device).eval()\n",
    "\n",
    "# Preprocessing transform for person crops\n",
    "transform = T.Compose([\n",
    "    T.Resize((256, 128)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225])])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9180e41-0f0a-4545-8cec-4e04325e0fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(frames_folder, detections_dict):\n",
    "    embeddings = {}\n",
    "    cap = cv2.VideoCapture(frames_folder.replace(\"frames_\", \"\") + \".mp4\")\n",
    "\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    cap.release()\n",
    "\n",
    "    for frame_idx, boxes in detections_dict.items():\n",
    "        frame_path = os.path.join(frames_folder, f\"frame_{frame_idx:05d}.jpg\")\n",
    "        if not os.path.exists(frame_path):\n",
    "            continue\n",
    "\n",
    "        frame = Image.open(frame_path).convert(\"RGB\")\n",
    "        person_embeddings = []\n",
    "\n",
    "        for (x1, y1, x2, y2) in boxes:\n",
    "            crop = frame.crop((x1, y1, x2, y2))\n",
    "            img_tensor = transform(crop).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                emb = reid_model(img_tensor)\n",
    "            emb = emb.squeeze().cpu().numpy()\n",
    "            emb = emb / np.linalg.norm(emb)\n",
    "            person_embeddings.append(emb)\n",
    "\n",
    "        embeddings[frame_idx] = person_embeddings\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "in_embeddings = extract_embeddings(\"frames_in\", in_detections)\n",
    "out_embeddings = extract_embeddings(\"frames_out\", out_detections)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "498c0441-48fb-475f-b5bf-2e8e758250d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Person_ID</th>\n",
       "      <th>Entry_Timestamp_Video</th>\n",
       "      <th>Exit_Timestamp_Video</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>6092</td>\n",
       "      <td>15379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>6098</td>\n",
       "      <td>14813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>6104</td>\n",
       "      <td>14517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>6105</td>\n",
       "      <td>3721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>6106</td>\n",
       "      <td>15258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2498</th>\n",
       "      <td>2499</td>\n",
       "      <td>20887</td>\n",
       "      <td>9964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>2500</td>\n",
       "      <td>20888</td>\n",
       "      <td>10510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2500</th>\n",
       "      <td>2501</td>\n",
       "      <td>20889</td>\n",
       "      <td>9953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2501</th>\n",
       "      <td>2502</td>\n",
       "      <td>20890</td>\n",
       "      <td>9924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2502</th>\n",
       "      <td>2503</td>\n",
       "      <td>20894</td>\n",
       "      <td>10514</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2503 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Person_ID  Entry_Timestamp_Video  Exit_Timestamp_Video\n",
       "0             1                   6092                 15379\n",
       "1             2                   6098                 14813\n",
       "2             3                   6104                 14517\n",
       "3             4                   6105                  3721\n",
       "4             5                   6106                 15258\n",
       "...         ...                    ...                   ...\n",
       "2498       2499                  20887                  9964\n",
       "2499       2500                  20888                 10510\n",
       "2500       2501                  20889                  9953\n",
       "2501       2502                  20890                  9924\n",
       "2502       2503                  20894                 10514\n",
       "\n",
       "[2503 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Match entry and exit persons\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "\n",
    "def match_people(in_embeds, out_embeds, threshold=0.6):\n",
    "    results = []\n",
    "    person_id = 1\n",
    "\n",
    "    for in_frame, in_vectors in in_embeds.items():\n",
    "        for in_vec in in_vectors:\n",
    "            best_match = None\n",
    "            best_score = -1\n",
    "            best_out_frame = None\n",
    "\n",
    "            for out_frame, out_vectors in out_embeds.items():\n",
    "                if len(out_vectors) == 0:\n",
    "                    continue\n",
    "                sims = cosine_similarity([in_vec], out_vectors)[0]\n",
    "                max_sim = np.max(sims)\n",
    "                if max_sim > best_score:\n",
    "                    best_score = max_sim\n",
    "                    best_out_frame = out_frame\n",
    "\n",
    "            if best_score >= threshold:\n",
    "                results.append({\n",
    "                    \"Person_ID\": person_id,\n",
    "                    \"Entry_Timestamp_Video\": in_frame,\n",
    "                    \"Exit_Timestamp_Video\": best_out_frame,})\n",
    "                \n",
    "                person_id += 1\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "matches_df = match_people(in_embeddings, out_embeddings)\n",
    "matches_df.to_csv(\"results.csv\", index=False)\n",
    "matches_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f68cd4c-ebc9-444e-b650-f81b01cdd8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotated video saved to:In_annotated.mp4\n",
      "Annotated video saved to:Out_annotated.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def annotate_video(video_path, detections, id_assignments, output_path):\n",
    "    \"\"\"Draw bounding boxes and IDs on a video.\n",
    "    detections: dict frame_idx - list of [x1, y1, x2, y2]\n",
    "    id_assignments: dict frame_idx - list of person IDs (same order as detections) \"\"\"\n",
    "   \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out_vid = cv2.VideoWriter(output_path,fourcc,fps,(width,height))\n",
    "\n",
    "    frame_idx = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        boxes = detections.get(frame_idx, [])\n",
    "        ids = id_assignments.get(frame_idx,[None]*len(boxes))\n",
    "\n",
    "        for (box, pid) in zip(boxes,ids):\n",
    "            x1,y1,x2,y2 = box\n",
    "            color = (0,255,0)\n",
    "            label = f\"ID {pid}\" if pid else \"Unknown\"\n",
    "            cv2.rectangle(frame,(x1,y1),(x2,y2),color,2)\n",
    "            cv2.putText(frame,label,(x1,y1-10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX,0.6,color,2)\n",
    "\n",
    "        out_vid.write(frame)\n",
    "        frame_idx += 1\n",
    "\n",
    "    cap.release()\n",
    "    out_vid.release()\n",
    "    print(f\"Annotated video saved to:{output_path}\")\n",
    "\n",
    "#create simple ID assignment mapping using your matches_df\n",
    "def generate_id_assignments(matches_df, detections):\n",
    "    \"\"\"\n",
    "    Generates frame-wise ID assignments based on matching results.\n",
    "    For simplicity: mark frames listed in matches_df with person IDs.\"\"\"\n",
    "    \n",
    "    id_assignments = {f:[None]*len(b) for f, b in detections.items()}\n",
    "\n",
    "    for _, row in matches_df.iterrows():\n",
    "        entry_frame = int(row[\"Entry_Timestamp_Video\"])\n",
    "        pid = int(row[\"Person_ID\"])\n",
    "        if entry_frame in id_assignments:\n",
    "            for i in range(len(id_assignments[entry_frame])):\n",
    "                id_assignments[entry_frame][i] = pid\n",
    "    return id_assignments\n",
    "\n",
    "#Run visualization for both videos\n",
    "in_id_assignments = generate_id_assignments(matches_df, in_detections)\n",
    "out_id_assignments = generate_id_assignments(matches_df, out_detections)\n",
    "\n",
    "annotate_video(\"In.mp4\", in_detections, in_id_assignments, \"In_annotated.mp4\")\n",
    "annotate_video(\"Out.mp4\", out_detections, out_id_assignments, \"Out_annotated.mp4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87e0952b-ab51-4368-aff2-dcd3c07544d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "\n",
    "# Initialize YOLOv8 for person detection\n",
    "model = YOLO(\"yolov8n.pt\") \n",
    "\n",
    "# Initialize DeepSORT tracker for smooth continuous IDs\n",
    "tracker = DeepSort(\n",
    "    max_age=30,              \n",
    "    n_init=3,                 \n",
    "    nms_max_overlap=1.0,\n",
    "    max_cosine_distance=0.3,  \n",
    "    embedder=\"mobilenet\",\n",
    "    half=True)\n",
    "\n",
    "def visualize_tracking(video_path, output_path):\n",
    "    \n",
    "    \"\"\" Run YOLO + DeepSORT on the full video and visualize smooth tracking. \"\"\"\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out_vid = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    frame_idx = 0\n",
    "    id_records = []\n",
    "\n",
    "    for _ in tqdm(range(total_frames), desc=f\"Processing {os.path.basename(video_path)}\"):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # YOLO person detection\n",
    "        results = model(frame, classes=[0], verbose=False)\n",
    "        detections = []\n",
    "        for box in results[0].boxes:\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "            conf = float(box.conf[0])\n",
    "            detections.append(([x1, y1, x2 - x1, y2 - y1], conf, 'person'))\n",
    "\n",
    "        # Update DeepSORT tracker\n",
    "        tracks = tracker.update_tracks(detections, frame=frame)\n",
    "\n",
    "        for track in tracks:\n",
    "            if not track.is_confirmed():\n",
    "                continue\n",
    "            track_id = track.track_id\n",
    "            l, t, r, b = map(int, track.to_ltrb())\n",
    "\n",
    "            # Draw bounding box + label\n",
    "            cv2.rectangle(frame, (l, t), (r, b), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f\"ID {track_id}\", (l, t - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "            # Record timestamp info\n",
    "            timestamp = frame_idx / fps\n",
    "            id_records.append([track_id, timestamp])\n",
    "\n",
    "        out_vid.write(frame)\n",
    "        frame_idx += 1\n",
    "\n",
    "    cap.release()\n",
    "    out_vid.release()\n",
    "\n",
    "    # Save tracking summary (first and last appearance per ID)\n",
    "    df = pd.DataFrame(id_records, columns=[\"Person_ID\", \"Timestamp\"])\n",
    "    summary = df.groupby(\"Person_ID\").agg(\n",
    "        Entry_Timestamp_Video=(\"Timestamp\", \"min\"),\n",
    "        Exit_Timestamp_Video=(\"Timestamp\", \"max\")).reset_index()\n",
    "\n",
    "    csv_path = os.path.splitext(output_path)[0] + \"_data.csv\"\n",
    "    summary.to_csv(csv_path, index=False)\n",
    "\n",
    "    print(f\" Full video processed: {output_path}\")\n",
    "    print(f\" Tracking summary saved: {csv_path}\")\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f338bb50-7449-4ce9-99e8-11f7ed637b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing In.mp4:  50%|██████████████████████████▍                          | 21436/42877 [3:24:09<3:24:11,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Full video processed: In_full_tracked.mp4\n",
      " Tracking summary saved: In_full_tracked_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Out.mp4: 100%|██████████████████████████████████████████████████████| 26997/26997 [2:30:42<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Full video processed: Out_full_tracked.mp4\n",
      " Tracking summary saved: Out_full_tracked_data.csv\n"
     ]
    }
   ],
   "source": [
    "in_summary = visualize_tracking(\"In.mp4\", \"In_full_tracked.mp4\")\n",
    "out_summary = visualize_tracking(\"Out.mp4\", \"Out_full_tracked.mp4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9fd637-e4c9-408b-bf73-7369cb885eec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
